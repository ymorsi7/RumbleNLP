{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn. model_selection import train_test_split\n",
    "from sklearn. tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    '''\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): a list of text documents.\n",
    "        n (int): number of top words to return.\n",
    "    '''\n",
    "    assert isinstance(corpus, list), \"This must be a list!\"\n",
    "    assert isinstance(n, int), \"This must be an integer!\"\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(corpus)\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[1]\n",
    "    df_tfidfvectorizer = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "\n",
    "    commentsTF_IDF = df_tfidfvectorizer.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    return commentsTF_IDF.head(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments = pd.read_csv('data/data.csv', encoding='utf-8')\n",
    "df = pd.DataFrame(comments)\n",
    "df.drop(['Number'], axis=1, inplace=True) # Drop the Number column (cleaning up the data)\n",
    "vid1 = vid2 = vid3 = vid4 = vid5 = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Find the Top 15 Words in Each Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 1: \"Women Should Not Be in Combat Roles: Change My Mind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tfidf\n",
      "more      0.270853\n",
      "be        0.241720\n",
      "injured   0.230368\n",
      "affected  0.230368\n",
      "would     0.214345\n",
      "those     0.208332\n",
      "their     0.206575\n",
      "woman     0.206575\n",
      "in        0.190632\n",
      "being     0.180569\n",
      "even      0.180569\n",
      "by        0.180569\n",
      "than      0.155024\n",
      "it        0.150067\n",
      "men       0.124610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymorsi/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vid1 = vid1[vid1.Video == 1]\n",
    "vid1List = vid1[\"Comment\"].values.tolist()\n",
    "print(get_top_n_words(vid1List, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 2: \"The Problem With Modern Women\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tfidf\n",
      "more      0.270853\n",
      "be        0.241720\n",
      "injured   0.230368\n",
      "affected  0.230368\n",
      "would     0.214345\n",
      "those     0.208332\n",
      "their     0.206575\n",
      "woman     0.206575\n",
      "in        0.190632\n",
      "being     0.180569\n",
      "even      0.180569\n",
      "by        0.180569\n",
      "than      0.155024\n",
      "it        0.150067\n",
      "men       0.124610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymorsi/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vid2 = df[df.Video == 2]\n",
    "vid2List = vid1[\"Comment\"].values.tolist()\n",
    "print(get_top_n_words(vid2List, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 3: \"Tucker Carlson Gives CNN Some Tips About Sexism in Hilarious Segment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tfidf\n",
      "more      0.270853\n",
      "be        0.241720\n",
      "injured   0.230368\n",
      "affected  0.230368\n",
      "would     0.214345\n",
      "those     0.208332\n",
      "their     0.206575\n",
      "woman     0.206575\n",
      "in        0.190632\n",
      "being     0.180569\n",
      "even      0.180569\n",
      "by        0.180569\n",
      "than      0.155024\n",
      "it        0.150067\n",
      "men       0.124610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymorsi/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vid3 = df[df.Video == 3]\n",
    "vid3List = vid1[\"Comment\"].values.tolist()\n",
    "print(get_top_n_words(vid3List, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 4: \"WOMAN DEFENDS ANDREW TATE AND ARGUES WITH FEMINISTS AND TRANGENDERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tfidf\n",
      "more      0.270853\n",
      "be        0.241720\n",
      "injured   0.230368\n",
      "affected  0.230368\n",
      "would     0.214345\n",
      "those     0.208332\n",
      "their     0.206575\n",
      "woman     0.206575\n",
      "in        0.190632\n",
      "being     0.180569\n",
      "even      0.180569\n",
      "by        0.180569\n",
      "than      0.155024\n",
      "it        0.150067\n",
      "men       0.124610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymorsi/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vid4 = df[df.Video == 4]\n",
    "vid4List = vid1[\"Comment\"].values.tolist()\n",
    "print(get_top_n_words(vid4List, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 5: \"Massive Feminist March Against Gender Violence in Rome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf\n",
      "their        0.37430\n",
      "truckers     0.20114\n",
      "leaders      0.20114\n",
      "freezing     0.20114\n",
      "efforts      0.20114\n",
      "govt         0.20114\n",
      "ottawa       0.20114\n",
      "bank         0.20114\n",
      "canada       0.20114\n",
      "least        0.20114\n",
      "arresting    0.20114\n",
      "fundraising  0.20114\n",
      "isn          0.20114\n",
      "accounts     0.20114\n",
      "associated   0.20114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymorsi/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vid5 = df[df.Video == 5]\n",
    "vid5List = vid5[\"Comment\"].values.tolist()\n",
    "print(get_top_n_words(vid5List, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 Words Overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tfidf\n",
      "be        0.245173\n",
      "injured   0.243024\n",
      "affected  0.243024\n",
      "more      0.234565\n",
      "would     0.223824\n",
      "woman     0.211882\n",
      "in        0.198536\n",
      "their     0.189318\n",
      "even      0.185259\n",
      "those     0.185259\n",
      "being     0.175961\n",
      "than      0.161941\n",
      "by        0.151469\n",
      "combat    0.149216\n",
      "it        0.145994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ymorsi/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Video'], axis=1, inplace=True) # Drop the video column (cleaning up the data)\n",
    "commentsList = df[\"Comment\"].values.tolist()\n",
    "print(get_top_n_words(commentsList, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Detect Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ymorsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk. download('stopwords')\n",
    "from nltk. corpus import stopwords\n",
    "from sklearn. metrics import accuracy_score\n",
    "stopword=set(stopwords.words('english'))\n",
    "stemmer = nltk. SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
     ]
    }
   ],
   "source": [
    "data = pd. read_csv(\"data/labeled_data.csv\")\n",
    "print(data. head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Pre-Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "\n",
      "                         labels  \n",
      "0  No Hate and Offensive Speech  \n",
      "1              Offensive Speech  \n",
      "2              Offensive Speech  \n",
      "3              Offensive Speech  \n",
      "4              Offensive Speech  \n"
     ]
    }
   ],
   "source": [
    "data[\"labels\"] = data[\"class\"]. map({0: \"Hate Speech\", 1: \"Offensive Speech\", 2: \"No Hate and Offensive Speech\"})\n",
    "data = data[[\"tweet\", \"labels\"]]\n",
    "print(data. head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = str(text). lower()\n",
    "    text = re.sub('[.?]', '', text) \n",
    "    text = re.sub('https?://\\S+|www.\\S+', '', text)\n",
    "    text = re.sub('<.?>+', '', text)\n",
    "    text = re.sub('[%s]' % re. escape(string. punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w\\d\\w', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text = \" \".join(text)\n",
    "    text = [stemmer. stem(word) for word in text. split(' ')]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         rt mayasolov woman shouldnt complain clean ho...\n",
       "1         rt mlee boy dat coldtyga dwn bad cuffin dat h...\n",
       "2         rt urkindofbrand dawg rt babif ever fuck bitc...\n",
       "3                   rt cganderson vivabas look like tranni\n",
       "4         rt shenikarobert shit hear might true might f...\n",
       "                               ...                        \n",
       "24778    yous muthafin lie 0lifeask earl coreyemanuel r...\n",
       "24779    youv gone broke wrong heart babi drove redneck...\n",
       "24780    young buck wanna eat dat nigguh like aint fuck...\n",
       "24781                       youu got wild bitch tellin lie\n",
       "24782    ruffl  ntac eileen dahlia  beauti color combin...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[\"tweet\"])\n",
    "y = np.array(data[\"labels\"])\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x)\n",
    "# Splitting the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model building\n",
    "model = DecisionTreeClassifier()\n",
    "#Training the model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Offensive Speech', 'Offensive Speech', 'Offensive Speech', ...,\n",
       "       'No Hate and Offensive Speech', 'No Hate and Offensive Speech',\n",
       "       'Offensive Speech'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8911847414109304\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Score of our model\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Accuracy Score Tells Us Our Model is 89% Accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Run the Model on Our Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Hate Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Hate Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Hate Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Hate Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Hate Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n",
      "['No Hate and Offensive Speech']\n"
     ]
    }
   ],
   "source": [
    "binClfr = []\n",
    "numHate = 0\n",
    "for i in range(len(commentsList)):\n",
    "    inp = cv.transform([commentsList[i]]).toarray()\n",
    "    if (model.predict(inp) == ['Offensive Speech']):\n",
    "        binClfr.append(1) # add one if offensive\n",
    "        numHate += 1\n",
    "    elif (model.predict(inp) == ['No Hate and Offensive Speech']):\n",
    "        binClfr.append(0) # add zero if comment is not hate speech\n",
    "    else:\n",
    "        binClfr.append(9) # Add 9 if output it neither (shouldn't happen; means that there's an error)\n",
    "    print(model.predict(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 9, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 9, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 9, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(binClfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "250\n",
      "Percentage of hate speech comments: 0.26\n"
     ]
    }
   ],
   "source": [
    "print(numHate)\n",
    "print(len(binClfr))\n",
    "\n",
    "print(\"Percentage of hate speech comments: \" + str(numHate/len(binClfr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Allows Us to Conclude That Over One Quarter of the Comments of the Top 50 Comments on Each Video Contain Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70143cde12a6018723f76b895bc6fa3c6c60a438dca19b64fe352303ed3314fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
